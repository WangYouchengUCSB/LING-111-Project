{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f692e-9680-4639-b3ae-70b3625b000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vectors\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# LOADING DATA - GENERAL\n",
    "\n",
    "class TSVDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"Loads the data from a provided filepath\"\"\"\n",
    "        self.data = list()\n",
    "        with open(filepath, encoding=\"utf-8\") as in_file:\n",
    "            for line in in_file:\n",
    "                (label, text) = line.strip().split(\"\\t\")\n",
    "                self.data.append((label, text))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns the datapoint at a given index\"\"\"\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of datapoints in the dataset\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "spacy_tokenizer = get_tokenizer('spacy', language=\"en_core_web_sm\")\n",
    "tokenizer = lambda text: [token.lower() for token in spacy_tokenizer(text)]\n",
    "    \n",
    "def text_to_indices(text):\n",
    "    tokens = tokenizer(text)\n",
    "    indices = vocab(tokens)\n",
    "    return torch.tensor(indices, dtype=torch.int64)\n",
    "\n",
    "def label_to_index(label):\n",
    "    return int(label == \"pos\")\n",
    "\n",
    "def data_to_indices(data):\n",
    "    (label, text) = data\n",
    "    return (label_to_index(label), text_to_indices(text))\n",
    "\n",
    "train_data = TSVDataset(\"inputs/imdb-train.tsv\")\n",
    "test_data = TSVDataset(\"inputs/imdb-test.tsv\")\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# SETTING UP THE VOCAB AND EMBEDDINGS - GENERAL\n",
    "\n",
    "def yield_tokens(data):\n",
    "    \"\"\"A generator for tokenizing text in a (label, text) pair\"\"\"\n",
    "    for _, text in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "tokenized_iter = yield_tokens(train_data)\n",
    "embeddings = Vectors(\"inputs/glove_6B_50_sample_train.txt\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# TRAINING AND TESTING - GENERAL\n",
    "\n",
    "def train(model, dataloader, optimizer, epochs=100, print_every=1,\n",
    "          validation_data=None):\n",
    "    \"\"\"Train a PyTorch model and print results periodically\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    model: torch.nn.Module; the model to be trained\n",
    "    dataloader: torch.utils.data.DataLoader; the training data\n",
    "    optimizer: the PyTorch optimizer to use for training\n",
    "    epochs: int; the number of complete cycles through the training data\n",
    "    print_every: int; print the results after this many epochs\n",
    "                 (does not print if this is None)\n",
    "    validation_data: torch.utils.data.DataLoader; the validation data\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if print_every is not None:\n",
    "        # Print initial performance\n",
    "        initial_performance = test(model, dataloader)\n",
    "        log_message = '| epoch   0 | train acc {acc:6.3f} | train loss {loss:6.3f} |'.format(**initial_performance)\n",
    "        if validation_data is not None:\n",
    "            validation_performance = test(model, validation_data)\n",
    "            log_message += ' valid acc {acc:6.3f} | valid loss {loss:6.3f} |'.format(**validation_performance)\n",
    "        print(log_message)\n",
    "        \n",
    "        # Set up trackers for printing results along the way\n",
    "        total_acc = 0\n",
    "        total_count = 0\n",
    "        current_loss = 0.0\n",
    "        minibatches_per_log = len(dataloader) * print_every\n",
    "        \n",
    "    # Tell the model that these inputs will be used for training\n",
    "    model.train()\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        # Within each epoch, iterate over the data in mini-batches\n",
    "        # Note the use of *datapoint_list for generality, whether or not there are offsets\n",
    "        for (label_list, *datapoint_list) in dataloader:\n",
    "            \n",
    "            # Clear out gradients accumulated from inputs in the previous mini-batch\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Run the forward pass to make predictions for the mini-batch\n",
    "            predicted_probs = model(*datapoint_list).view(-1)\n",
    "\n",
    "            # Compute the loss and send it backward through the network to get gradients\n",
    "            # Note: PyTorch averages the loss over all datapoints in the minibatch\n",
    "            loss = model.loss_function(predicted_probs, label_list.to(torch.float32))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Nudge the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track performance\n",
    "            if print_every is not None: \n",
    "                total_acc += ((predicted_probs > 0.5).to(torch.int64) == label_list).sum().item()\n",
    "                total_count += label_list.size(0)\n",
    "                current_loss += loss.item()\n",
    "\n",
    "        # Log performance\n",
    "        if print_every is not None and (epoch + 1) % print_every == 0:\n",
    "            log_message = ('| epoch {:3d} | train acc {:6.3f} | train loss {:6.3f} |'\n",
    "                           .format(epoch + 1, total_acc/total_count, current_loss/minibatches_per_log))\n",
    "            if validation_data is not None:\n",
    "                validation_performance = test(model, validation_data)\n",
    "                log_message += ' valid acc {acc:6.3f} | valid loss {loss:6.3f} |'.format(**validation_performance)\n",
    "            print(log_message)\n",
    "\n",
    "            # Reset trackers after logging\n",
    "            total_acc = 0\n",
    "            total_count = 0\n",
    "            current_loss = 0.0\n",
    "            model.train()\n",
    "            \n",
    "    print(\"\\nOverall training time: {:.0f} seconds\".format(time.time() - start_time))\n",
    "            \n",
    "def test(model, dataloader):\n",
    "    \"\"\"Evaluate a PyTorch model by testing it on labeled data\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    model: torch.nn.Module; the model to be tested\n",
    "    dataloader: torch.utils.data.DataLoader; the test data\n",
    "    \"\"\"\n",
    "    # Tell the model that these inputs will be used for evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    # Set up trackers\n",
    "    total_acc = 0\n",
    "    total_count = 0\n",
    "    loss = 0.0\n",
    "\n",
    "    with torch.no_grad(): # This can speed things up by telling PyTorch to ignore gradients\n",
    "        # Note the use of *datapoint_list for generality, whether or not there are offsets\n",
    "        for (label_list, *datapoint_list) in dataloader:\n",
    "            # Get the model's output predictions\n",
    "            predicted_probs = model(*datapoint_list).view(-1)\n",
    "            predicted_labels = (predicted_probs > 0.5).to(torch.int64)\n",
    "            \n",
    "            # Calculate the loss and accuracy\n",
    "            loss += model.loss_function(predicted_probs, label_list.to(torch.float32)).item()\n",
    "            total_acc += (predicted_labels == label_list).sum().item()\n",
    "            total_count += label_list.size(0)\n",
    "    \n",
    "    performance = {\"acc\": total_acc/total_count, \"loss\": loss/len(dataloader)}\n",
    "    return performance\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# INSPECTING A MODEL - GENERAL\n",
    "\n",
    "def display_weights(model):\n",
    "    \"\"\"Prints the weights of a model\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name.upper(), param)\n",
    "        print()\n",
    "        \n",
    "def predict_multiple(model, texts, collate_batch_fn, labels=[\"neg\", \"pos\"]):\n",
    "    \"\"\"Prints a model's predictions for a list of input texts.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    model: torch.nn.Module; a PyTorch RNN model\n",
    "    texts: list(str); a list of untokenized strings to feed as input to the model\n",
    "    collate_batch_fn: function; a function that is used to prepare (batched) data\n",
    "                      to be input into the model\n",
    "    labels: list(str); a list of the labels that correspond to the indices the\n",
    "            model will output\n",
    "    \"\"\"\n",
    "    # Tell the model not to use these inputs for training\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert the input texts to indices, and get other model arguments needed\n",
    "    data = [(None, text) for text in texts]\n",
    "    (_, *model_input) = collate_batch_fn(data)\n",
    "    \n",
    "    # Feed the inputs through the model\n",
    "    with torch.no_grad():\n",
    "        probs = model(*model_input).view(-1)\n",
    "    \n",
    "    # Collate the predictions in a DataFrame\n",
    "    predictions = pd.DataFrame({\"Input text\": texts, \"Classifier probability\": probs})\n",
    "    predictions[\"Output label\"] = labels[0]\n",
    "    predictions.loc[predictions[\"Classifier probability\"] > 0.5, \"Output label\"] = \"pos\"\n",
    "    return predictions\n",
    "\n",
    "        \n",
    "# =================================\n",
    "# LOADING DATA - SPECIFIC TO BOE\n",
    "\n",
    "def collate_batch_boe(batch):\n",
    "    \"\"\"Converts a batch of data into PyTorch tensor format, and collates\n",
    "    the results by label, text, and offset, for use in a bag-of-embeddings\n",
    "    model.\n",
    "    \"\"\"\n",
    "    # Initialize lists that separate out the three components\n",
    "    label_list = list()\n",
    "    text_list = list()\n",
    "    offsets_list = [0]\n",
    "    \n",
    "    for data in batch:\n",
    "        # Convert to PyTorch format\n",
    "        (label_index, text_indices) = data_to_indices(data)\n",
    "        # Add converted data to separate component lists\n",
    "        label_list.append(label_index)\n",
    "        text_list.append(text_indices)\n",
    "        offsets_list.append(text_indices.size(0))\n",
    "        \n",
    "    # Convert everything to tensors\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_tensor = torch.cat(text_list)\n",
    "    offsets_tensor = torch.tensor(offsets_list[:-1]).cumsum(dim=0)\n",
    "    \n",
    "    return (label_tensor, text_tensor, offsets_tensor)\n",
    "          \n",
    "train_dataloader_boe = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch_boe)\n",
    "test_dataloader_boe = DataLoader(test_data, batch_size=1000, collate_fn=collate_batch_boe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd973a1-2fbd-4464-a447-fc3da2ef5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, embeddings, recurrent_dim, hidden_dim, freeze_embeddings=True,\n",
    "                 recurrent_activation=\"tanh\", recurrent_layers=1, recurrent_bidirectional=False):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "        vocab_embeddings = embeddings.get_vecs_by_tokens(self.vocab.get_itos())\n",
    "        padding_idx = self.vocab.get_stoi().get(\"<pad>\")  # Get the <pad> index\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab_embeddings, freeze=freeze_embeddings, \n",
    "                                                      padding_idx=padding_idx) # Tell PyTorch that <pad> is for padding\n",
    "        \n",
    "        # The embeddings go into an RNN layer with recurrent_dim units\n",
    "        self.recurrent_layer = nn.RNN(embeddings.dim, recurrent_dim, nonlinearity=recurrent_activation,\n",
    "                                      num_layers=recurrent_layers, bidirectional=recurrent_bidirectional,\n",
    "                                      batch_first=True) # Because we'll make the mini-batch a list of sequences\n",
    "        \n",
    "        # The recurrent output creates a doc_embedding, which feeds into a of hidden_dim units\n",
    "        # We'll be concatenating the forward and backward direction of all layers\n",
    "        # from the recurrent output, so the doc_embedding will be sized accordingly\n",
    "        doc_embedding_dim = recurrent_dim * recurrent_layers * int(1 + recurrent_bidirectional)\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(doc_embedding_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # The output layer will go from the hidden layer (hidden_dim units) to a single unit\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.loss_function = nn.BCELoss()\n",
    "\n",
    "    def forward(self, padded_text, seq_lengths):\n",
    "        word_embeddings = self.embedding(padded_text)\n",
    "        \n",
    "        # The sequence of word embeddings has to be packed for efficiency of the RNN\n",
    "        packed_word_embeddings = pack_padded_sequence(word_embeddings, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "        (final_layer_all_timesteps, all_layers_final_timestep) = self.recurrent_layer(packed_word_embeddings)\n",
    "        \n",
    "        # all_layers_final_timestep contains the activations of all (stacked / bidirectional) recurrent\n",
    "        # layers at the final timestep for each sequence (taking the padding into account).\n",
    "        # For our classifier, we will stick all of these layers together (forward + backward, \n",
    "        # for each stacked layer) to use as the document embedding.\n",
    "        # all_layers_final_timestep has shape (num_layers, minibatch_size, recurrent_dim);\n",
    "        # we want something of shape (minibatch_size, num_layers * recurrent_dim),\n",
    "        # so we reorder the dimensions and then reshape to stick everything together\n",
    "        minibatch_size = all_layers_final_timestep.size(1)\n",
    "        doc_embedding = all_layers_final_timestep.permute(1, 0, 2).reshape(minibatch_size, -1)\n",
    "        \n",
    "        hidden = self.hidden_layer(doc_embedding)\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31224442-cf20-4833-ab22-4489d4fbb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(tokenized_iter, specials=[\"<pad>\", \"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "padding_idx = vocab.get_stoi().get(\"<pad>\")\n",
    "print(\"The <pad> symbol in this vocabulary is represented by the index {}\".format(padding_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ed96d4-27a2-44a9-912e-2b8c6c80e59a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m     text_tensor \u001b[38;5;241m=\u001b[39m pad_sequence(text_list, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39mpadding_idx)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (label_tensor, text_tensor, seq_lengths)\n\u001b[0;32m---> 25\u001b[0m train_dataloader_rnn \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_batch_rnn)\n\u001b[1;32m     26\u001b[0m test_dataloader_rnn \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_batch_rnn)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "def collate_batch_rnn(batch):\n",
    "    \"\"\"Converts a batch of sequence data into padded and packed PyTorch \n",
    "    tensor format, and collates the results by label, text, and sequence\n",
    "    length, for use in a RNN model.\n",
    "    \"\"\"\n",
    "    # Initialize lists that separate out the two components\n",
    "    label_list = list()\n",
    "    text_list = list()\n",
    "    seq_lengths = list()\n",
    "    \n",
    "    for data in batch:\n",
    "        # Convert to PyTorch format\n",
    "        (label_index, text_indices) = data_to_indices(data)\n",
    "        # Add converted data to separate component lists\n",
    "        label_list.append(label_index)\n",
    "        text_list.append(text_indices)\n",
    "        seq_lengths.append(len(text_indices))\n",
    "    \n",
    "    # Convert to mini-batch tensors\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_tensor = pad_sequence(text_list, batch_first=True, padding_value=padding_idx)\n",
    "    \n",
    "    return (label_tensor, text_tensor, seq_lengths)\n",
    "          \n",
    "train_dataloader_rnn = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch_rnn)\n",
    "test_dataloader_rnn = DataLoader(test_data, batch_size=1000, collate_fn=collate_batch_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cddca-4423-40d7-b471-0207290fe1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of padding\n",
    "texts = [\"I hated it\", \"it was quite terrible\", \"i really REALLY loved it\"]\n",
    "print(\"texts:\\n{}\\n\".format(texts))\n",
    "\n",
    "text_indices = [text_to_indices(text) for text in texts]\n",
    "print(\"Converted to indices:\\n{}\\n\".format(text_indices))\n",
    "\n",
    "padded = pad_sequence(text_indices, batch_first=True, padding_value=padding_idx)\n",
    "print(\"Padded:\\n{}\".format(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a96993-7de1-4181-9925-a1e261628ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # This tells PyTorch we aren't going to be doing backprop\n",
    "    \n",
    "    # Define simple embedding and recurrent layers.\n",
    "    # The embedding layer just contains the index of the input\n",
    "    # The recurrent layer has a bias of 1 and all weights fixed to 1,\n",
    "    # so it adds together all of the inputs up to the current timestep,\n",
    "    # plus the number of timesteps since the beginning of the sequence\n",
    "    embedding_layer = nn.Embedding.from_pretrained(torch.arange(5000, dtype=torch.float32).view(-1, 1), 1, padding_idx=padding_idx)\n",
    "    recurrent_layer = nn.RNN(1, 1, batch_first=True, nonlinearity=\"relu\")\n",
    "    recurrent_layer.bias_ih_l0[0] = 0.0\n",
    "    recurrent_layer.bias_hh_l0[0] = 1.0\n",
    "    recurrent_layer.weight_ih_l0[0, 0] = 1.0\n",
    "    recurrent_layer.weight_hh_l0[0, 0] = 1.0\n",
    "\n",
    "    # Get the word embeddings for the padded sequence\n",
    "    word_embeddings = embedding_layer(padded)\n",
    "\n",
    "    # With packing: pack the word embeddings, \n",
    "    # run the recurrent layer on the packed padded embeddings, \n",
    "    # and then unpack the results\n",
    "    packed_word_embeddings = pack_padded_sequence(word_embeddings, [3, 4, 5], batch_first=True, enforce_sorted=False)\n",
    "    (final_layer_all_timesteps, all_layers_final_timestep) = recurrent_layer(packed_word_embeddings)\n",
    "    final_layer_all_timesteps = pad_packed_sequence(final_layer_all_timesteps, batch_first=True, padding_value=padding_idx)\n",
    "\n",
    "    print(\"===========================\")\n",
    "    print(\"WITH PACKING\")\n",
    "    print(\"---------------------------\")\n",
    "    print(\"Recurrent layer activation at all timesteps, for each sequence:\")\n",
    "    print(final_layer_all_timesteps[0].view(3, -1))\n",
    "    print(\"\\nWhat PyTorch gets as the recurrent layer activation at the \\\"final\\\" timestep for each sequence:\")\n",
    "    for (seqnum, value) in enumerate(list(all_layers_final_timestep.view(-1))):\n",
    "        print(\"Sequence {}: {}\".format(seqnum + 1, int(value)))\n",
    "\n",
    "    # Without packing: run the recurrent layer on the padded embeddings directly\n",
    "    (final_layer_all_timesteps, all_layers_final_timestep) = recurrent_layer(word_embeddings)\n",
    "\n",
    "    print(\"\\n===========================\")\n",
    "    print(\"WITHOUT PACKING\")\n",
    "    print(\"---------------------------\")\n",
    "    print(\"Recurrent layer activation at all timesteps, for each sequence:\")\n",
    "    print(final_layer_all_timesteps.view(3, -1))\n",
    "    print(\"\\nWhat PyTorch gets as the recurrent layer activation at the \\\"final\\\" timestep for each sequence:\")\n",
    "    for (seqnum, value) in enumerate(list(all_layers_final_timestep.view(-1))):\n",
    "        print(\"Sequence {}: {}\".format(seqnum + 1, int(value)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ling111] *",
   "language": "python",
   "name": "conda-env-ling111-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
