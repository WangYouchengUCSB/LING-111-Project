{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f36385de-d003-4199-83b6-11e564d982bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5016, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch, torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "\n",
    "with open(\"/Users/clarkwang/Documents/GitHub/LING_111/Project/preprocessed_data/bnc_spoken_output_flattened.json\") as infile:\n",
    "    input_data = json.load(infile)\n",
    "    text = input_data[0]\n",
    "\n",
    "# print(text)\n",
    "\n",
    "\n",
    "seq_len = len(text)\n",
    "batch_size = 1\n",
    "embedding_size = 1\n",
    "hidden_size = 1\n",
    "\n",
    "# get max sequence length, then pad every sentence\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for sentence in input_data[0:10]:\n",
    "    vocab = vocab.union(sentence)\n",
    "\n",
    "vocab = list(vocab)\n",
    "vocab_size = len(vocab)\n",
    "output_size = vocab_size\n",
    "word_ids = {word: id_ for id_, word in enumerate(vocab)}\n",
    "ids_word = {id_: word for word, id_ in word_ids.items()}\n",
    "\n",
    "# text = input_data[random.randint(0, vocab_size - 1)]\n",
    "indices = [word_ids[word] for word in text]\n",
    "input_val = torch.eye(vocab_size)[indices].unsqueeze(1)\n",
    "\n",
    "bi_rnn = torch.nn.RNN(\n",
    "    input_size=vocab_size, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "bi_output, bi_hidden = bi_rnn(input_val)\n",
    "\n",
    "# stagger\n",
    "forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\n",
    "staggered_output = torch.cat((forward_output, backward_output), dim=-1)\n",
    "\n",
    "linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "# only predict on words\n",
    "labels = torch.LongTensor(indices[1:-1])\n",
    "\n",
    "# for language models, use cross-entropy :)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "output = loss(linear(staggered_output.squeeze(1)), labels)\n",
    "\n",
    "print(output)\n",
    "\n",
    "optimizer = optim.Adam(rnn_classifier_newstart_adam.parameters())\n",
    "\n",
    "'''\n",
    "for epoch in range(epochs):\n",
    "    # Within each epoch, iterate over the data in mini-batches\n",
    "    # Note the use of *datapoint_list for generality, whether or not there are offsets\n",
    "    for (label_list, *datapoint_list) in dataloader:\n",
    "            \n",
    "        # Clear out gradients accumulated from inputs in the previous mini-batch\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Run the forward pass to make predictions for the mini-batch\n",
    "        predicted_probs = model(*datapoint_list).view(-1)\n",
    "\n",
    "        # Compute the loss and send it backward through the network to get gradients\n",
    "        # Note: PyTorch averages the loss over all datapoints in the minibatch\n",
    "        loss = model.loss_function(predicted_probs, label_list.to(torch.float32))\n",
    "        loss.backward()\n",
    "            \n",
    "        # Nudge the weights\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Track performance\n",
    "        if print_every is not None: \n",
    "            total_acc += ((predicted_probs > 0.5).to(torch.int64) == label_list).sum().item()\n",
    "            total_count += label_list.size(0)\n",
    "            current_loss += loss.item()\n",
    "\n",
    "        # Log performance\n",
    "        if print_every is not None and (epoch + 1) % print_every == 0:\n",
    "            log_message = ('| epoch {:3d} | train loss {:6.3f} |'\n",
    "                           .format(epoch + 1, total_acc/total_count, current_loss/minibatches_per_log))\n",
    "            if validation_data is not None:\n",
    "                validation_performance = test(model, validation_data)\n",
    "                log_message += ' valid loss {loss:6.3f} |'.format(**validation_performance)\n",
    "            print(log_message)\n",
    "\n",
    "            # Reset trackers after logging\n",
    "            total_acc = 0\n",
    "            total_count = 0\n",
    "            current_loss = 0.0\n",
    "            model.train()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb5ca89a-303d-4892-a54a-26a0bc689b98",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'does'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBOS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEOS\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Ensure minimum length for stagger\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# One-hot encode\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m indices \u001b[38;5;241m=\u001b[39m [word2idx[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[1;32m     23\u001b[0m one_hot_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(vocab_size)[indices]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [seq_len, 1, vocab_size]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Define model\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'does'"
     ]
    }
   ],
   "source": [
    "class biRNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, hidden_size, freeze_embeddings=True,\n",
    "                 recurrent_activation=\"tanh\", recurrent_layers=1, recurrent_bidirectional=False):\n",
    "        super(biRNNLM, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, 10)\n",
    "        \n",
    "        # The embeddings go into an RNN layer with recurrent_dim units\n",
    "        self.bi_rnn = torch.nn.RNN(input_size=vocab_size, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, text, seq_lengths):\n",
    "        \n",
    "        indices = [word_ids[word] for word in text]\n",
    "        input_val = torch.eye(vocab_size)[indices].unsqueeze(1)\n",
    "        \n",
    "        word_embeddings = self.embedding(input_val)\n",
    "        \n",
    "        bi_output, bi_hidden = self.bi_rnn(word_embeddings)\n",
    "\n",
    "        # stagger\n",
    "        forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\n",
    "        staggered_output = torch.cat((forward_output, backward_output), dim=-1)\n",
    "\n",
    "        # only predict on words\n",
    "        labels = torch.LongTensor(indices[1:-1])\n",
    "\n",
    "        # for language models, use cross-entropy :)\n",
    "\n",
    "        output = self.loss_function(self.linear(staggered_output.squeeze(1)), labels)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5726c8-d13b-408c-be9b-29b78d0f18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Within each epoch, iterate over the data in mini-batches\n",
    "    # Note the use of *datapoint_list for generality, whether or not there are offsets\n",
    "    for (label_list, *datapoint_list) in dataloader:\n",
    "            \n",
    "        # Clear out gradients accumulated from inputs in the previous mini-batch\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Run the forward pass to make predictions for the mini-batch\n",
    "        predicted_probs = model(*datapoint_list).view(-1)\n",
    "\n",
    "        # Compute the loss and send it backward through the network to get gradients\n",
    "        # Note: PyTorch averages the loss over all datapoints in the minibatch\n",
    "        loss = model.loss_function(predicted_probs, label_list.to(torch.float32))\n",
    "        loss.backward()\n",
    "            \n",
    "        # Nudge the weights\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Track performance\n",
    "        if print_every is not None: \n",
    "            total_acc += ((predicted_probs > 0.5).to(torch.int64) == label_list).sum().item()\n",
    "            total_count += label_list.size(0)\n",
    "            current_loss += loss.item()\n",
    "\n",
    "        # Log performance\n",
    "        if print_every is not None and (epoch + 1) % print_every == 0:\n",
    "            log_message = ('| epoch {:3d} | train loss {:6.3f} |'\n",
    "                           .format(epoch + 1, total_acc/total_count, current_loss/minibatches_per_log))\n",
    "            if validation_data is not None:\n",
    "                validation_performance = test(model, validation_data)\n",
    "                log_message += ' valid loss {loss:6.3f} |'.format(**validation_performance)\n",
    "            print(log_message)\n",
    "\n",
    "            # Reset trackers after logging\n",
    "            total_acc = 0\n",
    "            total_count = 0\n",
    "            current_loss = 0.0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965c16c-37fe-4d8b-a348-dd15eb0feb16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ling111] *",
   "language": "python",
   "name": "conda-env-ling111-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
