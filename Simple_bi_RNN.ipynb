{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f36385de-d003-4199-83b6-11e564d982bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch, torch.nn as nn\n",
    "from torch import nn, optim, tensor\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "filepath = \"/Users/willi/OneDrive/Documents/GitHub/LING-111-Project/preprocessed_data/bnc_spoken_output_flattened.json\"\n",
    "\n",
    "sample_size = 1000\n",
    "with open(filepath) as infile:\n",
    "    input_data = json.load(infile)\n",
    "    text = random.sample(input_data, k=sample_size)\n",
    "\n",
    "label_dict = {\"transitive\": [], \"ditransitive\": []}\n",
    "\n",
    "for index, line in enumerate(text):\n",
    "    sentence = \" \".join(line)\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.pos_ != \"VERB\":\n",
    "            continue  # Use continue instead of pass to skip non-verbs\n",
    "\n",
    "        has_dobj = any(child.dep_ == \"dobj\" for child in token.children)\n",
    "        has_indirect = any(child.dep_ in {\"iobj\", \"obl\", \"prep\", \"dative\"} for child in token.children)\n",
    "\n",
    "        if has_dobj and has_indirect:\n",
    "            label_dict[\"ditransitive\"].append(token.text)\n",
    "        elif has_dobj:\n",
    "            label_dict[\"transitive\"].append(token.text)\n",
    "\n",
    "transitive_verbs = list(set(label_dict[\"transitive\"]))\n",
    "ditransitive_verbs = list(set(label_dict[\"ditransitive\"]))\n",
    "\n",
    "seq_len = len(text)\n",
    "batch_size = 1\n",
    "embedding_size = 100\n",
    "hidden_size = 128\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for sentence in text:\n",
    "    vocab = vocab.union(sentence)\n",
    "\n",
    "vocab.update([\"<unk>\", \"<pad>\"])\n",
    "\n",
    "vocab = list(vocab)\n",
    "vocab_size = len(vocab)\n",
    "verb_words = [word for word in vocab if nlp(word)[0].pos_ == \"VERB\"]\n",
    "verb_words = [word for word in verb_words if word not in (\"<unk>\", \"<pad>\")]\n",
    "\n",
    "output_size = vocab_size\n",
    "word_ids = {word: id_ for id_, word in enumerate(vocab)}\n",
    "ids_word = {id_: word for word, id_ in word_ids.items()}\n",
    "\n",
    "padding_idx = word_ids.get(\"<pad>\")\n",
    "max_len = max(len(sentence) for sentence in text)\n",
    "\n",
    "indexed_sentences = [\n",
    "    torch.LongTensor([word_ids.get(word, word_ids[\"<unk>\"]) for word in sentence])\n",
    "    for sentence in text \n",
    "]\n",
    "\n",
    "text_tensor = pad_sequence(indexed_sentences, batch_first=True, padding_value=padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fb5ca89a-303d-4892-a54a-26a0bc689b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class biRNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, hidden_size, freeze_embeddings=True,\n",
    "                 recurrent_activation=\"tanh\", recurrent_layers=1, recurrent_bidirectional=False):\n",
    "        super(biRNNLM, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, 10)\n",
    "        \n",
    "        self.bi_rnn = torch.nn.RNN(input_size=10, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, text=text_tensor, seq_lengths=max_len):\n",
    "        unk_index = word_ids.get('<unk>')  # fallback index\n",
    "        indices = torch.LongTensor([\n",
    "        word_ids[word] if word in word_ids else unk_index\n",
    "        for word in text\n",
    "        ]).unsqueeze(1)\n",
    "        word_embeddings = self.embedding(indices)\n",
    "        \n",
    "        bi_output, bi_hidden = self.bi_rnn(word_embeddings)\n",
    "\n",
    "        # stagger\n",
    "        forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\n",
    "        staggered_output = torch.cat((forward_output, backward_output), dim=-1)\n",
    "\n",
    "        labels = torch.LongTensor(indices[1:-1])\n",
    "\n",
    "        logits = self.linear(staggered_output.squeeze(1))  # expected shape [T, vocab_size]       \n",
    "        target = labels.squeeze(1).long()\n",
    "        loss = self.loss_function(logits, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8c5726c8-d13b-408c-be9b-29b78d0f18da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | train loss  5.942 |\n",
      "| epoch   2 | train loss  4.917 |\n",
      "| epoch   3 | train loss  4.219 |\n",
      "| epoch   4 | train loss  3.596 |\n",
      "| epoch   5 | train loss  3.107 |\n",
      "| epoch   6 | train loss  2.759 |\n",
      "| epoch   7 | train loss  2.500 |\n",
      "| epoch   8 | train loss  2.289 |\n",
      "| epoch   9 | train loss  2.108 |\n",
      "| epoch  10 | train loss  1.949 |\n"
     ]
    }
   ],
   "source": [
    "model = biRNNLM(vocab=vocab, hidden_size=hidden_size)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def train(model=model, optimizer=optimizer, epochs=10, print_every=1,\n",
    "          validation_data=None):\n",
    "    current_loss = 0.0\n",
    "    minibatches_per_log = len(text)\n",
    "    for epoch in range(epochs):\n",
    "        # Within each epoch, iterate over the data in mini-batches\n",
    "        # Note the use of *datapoint_list for generality, whether or not there are offsets\n",
    "        for sentence in text:\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            loss = model(sentence)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_every is not None:\n",
    "                current_loss += loss.item()\n",
    "\n",
    "        if print_every is not None and (epoch + 1) % print_every == 0:\n",
    "            log_message = ('| epoch {:3d} | train loss {:6.3f} |'\n",
    "                             .format(epoch + 1, current_loss/minibatches_per_log))\n",
    "            if validation_data is not None:\n",
    "                validation_performance = test(model, validation_data)\n",
    "                log_message += 'valid loss {loss:6.3f} |'.format(**validation_performance)\n",
    "            print(log_message)\n",
    "\n",
    "            current_loss = 0.0\n",
    "            model.train()\n",
    "train(model, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9c5f08c8-3ccf-4a7a-89c9-91b955e51cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embedding.weight.data\n",
    "\n",
    "def save_embeddings(word_list, label):\n",
    "    with open(f\"{label}_verb_embeddings.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for word in word_list:\n",
    "            idx = word_ids[word]\n",
    "            emb = embeddings[idx]\n",
    "            emb_str = \" \".join(f\"{v:.4f}\" for v in emb)\n",
    "            f.write(f\"{word}\\t{emb_str}\\n\")\n",
    "\n",
    "save_embeddings(transitive_verbs, \"transitive\")\n",
    "save_embeddings(ditransitive_verbs, \"ditransitive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61a1af-f805-4365-bb86-1ea1aff75dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ling111]",
   "language": "python",
   "name": "conda-env-ling111-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
