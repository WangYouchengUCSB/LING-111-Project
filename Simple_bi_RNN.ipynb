{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f36385de-d003-4199-83b6-11e564d982bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch, torch.nn as nn\n",
    "from torch import nn, optim, tensor\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "filepath = \"/Users/willi/OneDrive/Documents/GitHub/LING-111-Project/preprocessed_data/bnc_spoken_output_flattened.json\"\n",
    "\n",
    "with open(filepath) as infile:\n",
    "    input_data = json.load(infile)\n",
    "    text = input_data[:1000]\n",
    "\n",
    "seq_len = len(text)\n",
    "batch_size = 1\n",
    "embedding_size = 50\n",
    "hidden_size = 100\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for sentence in text:\n",
    "    vocab = vocab.union(sentence)\n",
    "\n",
    "vocab.update([\"<unk>\", \"<pad>\"])\n",
    "\n",
    "vocab = list(vocab)\n",
    "vocab_size = len(vocab)\n",
    "output_size = vocab_size\n",
    "word_ids = {word: id_ for id_, word in enumerate(vocab)}\n",
    "ids_word = {id_: word for word, id_ in word_ids.items()}\n",
    "padding_idx = word_ids.get(\"<pad>\")\n",
    "max_len = max(len(sentence) for sentence in text)\n",
    "\n",
    "indexed_sentences = [\n",
    "    torch.LongTensor([word_ids.get(word, word_ids[\"<unk>\"]) for word in sentence])\n",
    "    for sentence in text \n",
    "]\n",
    "\n",
    "text_tensor = pad_sequence(indexed_sentences, batch_first=True, padding_value=padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb5ca89a-303d-4892-a54a-26a0bc689b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class biRNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, hidden_size, freeze_embeddings=True,\n",
    "                 recurrent_activation=\"tanh\", recurrent_layers=1, recurrent_bidirectional=False):\n",
    "        super(biRNNLM, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, 10)\n",
    "        \n",
    "        # The embeddings go into an RNN layer with recurrent_dim units\n",
    "        self.bi_rnn = torch.nn.RNN(input_size=10, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, text=text_tensor, seq_lengths=max_len):\n",
    "        unk_index = word_ids.get('<unk>')  # fallback index\n",
    "        indices = torch.LongTensor([\n",
    "        word_ids[word] if word in word_ids else unk_index\n",
    "        for word in text\n",
    "        ]).unsqueeze(1)\n",
    "        word_embeddings = self.embedding(indices)\n",
    "        \n",
    "        bi_output, bi_hidden = self.bi_rnn(word_embeddings)\n",
    "\n",
    "        # stagger\n",
    "        forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\n",
    "        staggered_output = torch.cat((forward_output, backward_output), dim=-1)\n",
    "\n",
    "        # only predict on words\n",
    "        labels = torch.LongTensor(indices[1:-1])\n",
    "\n",
    "        logits = self.linear(staggered_output.squeeze(1))  # expected shape [T, vocab_size]       \n",
    "        target = labels.squeeze(1).long()\n",
    "        loss = self.loss_function(logits, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5726c8-d13b-408c-be9b-29b78d0f18da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | train loss  5.797 |\n",
      "| epoch   2 | train loss  4.864 |\n",
      "| epoch   3 | train loss  4.332 |\n",
      "| epoch   4 | train loss  3.863 |\n"
     ]
    }
   ],
   "source": [
    "model = biRNNLM(vocab=vocab, hidden_size=hidden_size)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def train(model=model, optimizer=optimizer, epochs=10, print_every=1,\n",
    "          validation_data=None):\n",
    "    current_loss = 0.0\n",
    "    minibatches_per_log = len(text)\n",
    "    for epoch in range(epochs):\n",
    "        # Within each epoch, iterate over the data in mini-batches\n",
    "        # Note the use of *datapoint_list for generality, whether or not there are offsets\n",
    "        for sentence in text:\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            loss = model(sentence)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_every is not None:\n",
    "                current_loss += loss.item()\n",
    "\n",
    "            # Log performance\n",
    "        if print_every is not None and (epoch + 1) % print_every == 0:\n",
    "            log_message = ('| epoch {:3d} | train loss {:6.3f} |'\n",
    "                             .format(epoch + 1, current_loss/minibatches_per_log))\n",
    "            if validation_data is not None:\n",
    "                validation_performance = test(model, validation_data)\n",
    "                log_message += 'valid loss {loss:6.3f} |'.format(**validation_performance)\n",
    "            print(log_message)\n",
    "\n",
    "            # Reset trackers after logging\n",
    "            current_loss = 0.0\n",
    "            model.train()\n",
    "train(model, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f08c8-3ccf-4a7a-89c9-91b955e51cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ling111]",
   "language": "python",
   "name": "conda-env-ling111-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
