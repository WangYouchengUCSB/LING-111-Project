{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36385de-d003-4199-83b6-11e564d982bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# text = input_data[random.randint(0, vocab_size - 1)]\\nindices = [word_ids[word] for word in text]\\ninput_val = torch.eye(vocab_size)[indices].unsqueeze(1)\\n\\nbi_rnn = torch.nn.RNN(\\n    input_size=vocab_size, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\\n\\nbi_output, bi_hidden = bi_rnn(input_val)\\n\\n# stagger\\nforward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\\nstaggered_output = torch.cat((forward_output, backward_output), dim=-1)\\n\\nlinear = nn.Linear(hidden_size * 2, output_size)\\n\\n# only predict on words\\nlabels = torch.LongTensor(indices[1:-1])\\n\\n# for language models, use cross-entropy :)\\nloss = nn.CrossEntropyLoss()\\noutput = loss(linear(staggered_output.squeeze(1)), labels)\\n\\nprint(output)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch, torch.nn as nn\n",
    "from torch import nn, optim, tensor\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "filepath = \".../bnc_spoken_output_flattened.json\"\n",
    "\n",
    "    input_data = json.load(infile)\n",
    "    text = input_data[:10]\n",
    "\n",
    "# print(text)\n",
    "\n",
    "\n",
    "seq_len = len(text)\n",
    "batch_size = 1\n",
    "embedding_size = 1\n",
    "hidden_size = 1\n",
    "\n",
    "# get max sequence length, then pad every sentence\n",
    "max_len = max(len(sentence) for sentence in input_data)\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for sentence in input_data[0:10]:\n",
    "    vocab = vocab.union(sentence)\n",
    "\n",
    "vocab.add(\"<unk>\")\n",
    "\n",
    "vocab = list(vocab)\n",
    "vocab_size = len(vocab)\n",
    "output_size = vocab_size\n",
    "word_ids = {word: id_ for id_, word in enumerate(vocab)}\n",
    "ids_word = {id_: word for word, id_ in word_ids.items()}\n",
    "'''\n",
    "# text = input_data[random.randint(0, vocab_size - 1)]\n",
    "indices = [word_ids[word] for word in text]\n",
    "input_val = torch.eye(vocab_size)[indices].unsqueeze(1)\n",
    "\n",
    "bi_rnn = torch.nn.RNN(\n",
    "    input_size=vocab_size, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "bi_output, bi_hidden = bi_rnn(input_val)\n",
    "\n",
    "# stagger\n",
    "forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\n",
    "staggered_output = torch.cat((forward_output, backward_output), dim=-1)\n",
    "\n",
    "linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "# only predict on words\n",
    "labels = torch.LongTensor(indices[1:-1])\n",
    "\n",
    "# for language models, use cross-entropy :)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "output = loss(linear(staggered_output.squeeze(1)), labels)\n",
    "\n",
    "print(output)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb5ca89a-303d-4892-a54a-26a0bc689b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class biRNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, hidden_size, freeze_embeddings=True,\n",
    "                 recurrent_activation=\"tanh\", recurrent_layers=1, recurrent_bidirectional=False):\n",
    "        super(biRNNLM, self).__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, 10)\n",
    "        \n",
    "        # The embeddings go into an RNN layer with recurrent_dim units\n",
    "        self.bi_rnn = torch.nn.RNN(input_size=10, hidden_size=hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, text, seq_lengths=max_len):\n",
    "        unk_index = word_ids.get('<unk>')  # fallback index\n",
    "        indices = torch.LongTensor([\n",
    "        word_ids[word] if word in word_ids else unk_index\n",
    "        for word in text\n",
    "        ]).unsqueeze(1)\n",
    "        word_embeddings = self.embedding(indices)\n",
    "        \n",
    "        bi_output, bi_hidden = self.bi_rnn(word_embeddings)\n",
    "\n",
    "        # stagger\n",
    "        forward_output, backward_output = bi_output[:-2, :, :hidden_size], bi_output[2:, :, hidden_size:]\n",
    "        staggered_output = torch.cat((forward_output, backward_output), dim=-1)\n",
    "\n",
    "        # only predict on words\n",
    "        labels = torch.LongTensor(indices[1:-1])\n",
    "\n",
    "        # for language models, use cross-entropy :)\n",
    "\n",
    "        output = self.loss_function(self.linear(staggered_output.squeeze(1)), labels.view(-1))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5726c8-d13b-408c-be9b-29b78d0f18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = biRNNLM(vocab=vocab, hidden_size=hidden_size)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def train(model=model, optimizer=optimizer, epochs=10, print_every=1,\n",
    "          validation_data=None):\n",
    "    current_loss = 0.0\n",
    "    minibatches_per_log = len(input_data)\n",
    "    for epoch in range(epochs):\n",
    "        # Within each epoch, iterate over the data in mini-batches\n",
    "        # Note the use of *datapoint_list for generality, whether or not there are offsets\n",
    "        for text in input_data:\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            loss = model(text)  # already returns the loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if print_every is not None:\n",
    "                current_loss += loss.item()\n",
    "\n",
    "            # Log performance\n",
    "        if print_every is not None and (epoch + 1) % print_every == 0:\n",
    "            log_message = ('| epoch {:3d} | train loss {:6.3f} |'\n",
    "                             .format(epoch + 1, current_loss/minibatches_per_log))\n",
    "            if validation_data is not None:\n",
    "                validation_performance = test(model, validation_data)\n",
    "                log_message += 'valid loss {loss:6.3f} |'.format(**validation_performance)\n",
    "            print(log_message)\n",
    "\n",
    "            # Reset trackers after logging\n",
    "            current_loss = 0.0\n",
    "            model.train()\n",
    "train(model, optimizer, 10, 1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308627ad-d25f-433f-b9df-436271b268c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ling111]",
   "language": "python",
   "name": "conda-env-ling111-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
